{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to build on the template from `00_interfacing_DEHB` and use it on an actual problem, to optimize the hyperparameters of a Random Forest model, for a dataset. Here we use DEHB with the built-in ask and tell functionality.\n",
    "\n",
    "Additional requirements:\n",
    "* scikit-learn>=0.24\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem defined here is to optimize a [Random Forest model](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), on any given dataset, using DEHB. The hyperparameters chosen to be optimized are:\n",
    "* `max_depth`\n",
    "* `min_samples_split`\n",
    "* `max_features`\n",
    "* `min_samples_leaf`\n",
    "while the `n_estimators` hyperparameter to the Random Forest is chosen to be a fidelity parameter instead. Lesser number of trees ($<10$) in the Random Forest may not allow adequate ensembling for the grouped prediction to be significantly better than the individual tree predictions. Whereas a large number of trees (~$100$) often give accurate predictions but is naturally slower to train and predict on account of more trees to train. Therefore, a smaller `n_estimators` can be used as a cheaper approximation of the actual fidelity of `n_estimators=100`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining fidelity range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_fidelity, max_fidelity = 2, 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining $4$ hyperparameters, the search space can be created as a `ConfigSpace` object, with the domain of individual parameters defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ConfigSpace as CS\n",
    "\n",
    "\n",
    "def create_search_space(seed=123):\n",
    "    \"\"\"Parameter space to be optimized --- contains the hyperparameters\n",
    "    \"\"\"\n",
    "    cs = CS.ConfigurationSpace(seed=seed)\n",
    "\n",
    "    cs.add_hyperparameters([\n",
    "        CS.UniformIntegerHyperparameter(\n",
    "            'max_depth', lower=1, upper=15, default_value=2, log=False\n",
    "        ),\n",
    "        CS.UniformIntegerHyperparameter(\n",
    "            'min_samples_split', lower=2, upper=128, default_value=2, log=True\n",
    "        ),\n",
    "        CS.UniformFloatHyperparameter(\n",
    "            'max_features', lower=0.1, upper=0.9, default_value=0.5, log=False\n",
    "        ),\n",
    "        CS.UniformIntegerHyperparameter(\n",
    "            'min_samples_leaf', lower=1, upper=64, default_value=1, log=True\n",
    "        ),\n",
    "    ])\n",
    "    return cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration space object:\n",
      "  Hyperparameters:\n",
      "    max_depth, Type: UniformInteger, Range: [1, 15], Default: 2\n",
      "    max_features, Type: UniformFloat, Range: [0.1, 0.9], Default: 0.5\n",
      "    min_samples_leaf, Type: UniformInteger, Range: [1, 64], Default: 1, on log-scale\n",
      "    min_samples_split, Type: UniformInteger, Range: [2, 128], Default: 2, on log-scale\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cs = create_search_space(seed)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of search space: 4\n"
     ]
    }
   ],
   "source": [
    "dimensions = len(cs.get_hyperparameters())\n",
    "print(\"Dimensionality of search space: {}\".format(dimensions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the primary black/gray-box interface to the Random Forest model needs to be built for DEHB to query. As given in the `00_interfacing_DEHB` notebook, this function will have a signature akin to: `target_function(config, fidelity)`, and return a two-element tuple of the `score` and `cost`. It must be noted that DEHB **minimizes** and therefore the `score` being returned by this `target_function` should account for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the target function trains a Random Forest model on a dataset. We load a dataset here and maintain a fixed, train-validation-test split for one complete run. Multiple DEHB runs can therefore optimize on the same validation split, and evaluate final performance on the same test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating target function to optimize (2 parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 ) Preparing dataset and splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_digits, load_wine\n",
    "\n",
    "\n",
    "classification = {\"iris\": load_iris, \"digits\": load_digits, \"wine\": load_wine}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def prepare_dataset(model_type=\"classification\", dataset=None):\n",
    "\n",
    "    if model_type == \"classification\":\n",
    "        if dataset is None:\n",
    "            dataset = np.random.choice(list(classification.keys())) \n",
    "        _data = classification[dataset]()\n",
    "    else:\n",
    "        if dataset is None:\n",
    "            dataset = np.random.choice(list(regression.keys()))\n",
    "        _data = regression[dataset]()\n",
    "\n",
    "    train_X, rest_X, train_y, rest_y = train_test_split(\n",
    "      _data.get(\"data\"), \n",
    "      _data.get(\"target\"), \n",
    "      train_size=0.7, \n",
    "      shuffle=True, \n",
    "      random_state=seed\n",
    "    )\n",
    "    \n",
    "    # 10% test and 20% validation data\n",
    "    valid_X, test_X, valid_y, test_y = train_test_split(\n",
    "      rest_X, rest_y,\n",
    "      test_size=0.3333, \n",
    "      shuffle=True, \n",
    "      random_state=seed\n",
    "    )\n",
    "    return train_X, train_y, valid_X, valid_y, test_X, test_y, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine\n",
      "Train size: (124, 13)\n",
      "Valid size: (36, 13)\n",
      "Test size: (18, 13)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y, valid_X, valid_y, test_X, test_y, dataset = \\\n",
    "    prepare_dataset(model_type=\"classification\")\n",
    "\n",
    "print(dataset)\n",
    "print(\"Train size: {}\\nValid size: {}\\nTest size: {}\".format(\n",
    "    train_X.shape, valid_X.shape, test_X.shape\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 ) Function interface with DEHB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "\n",
    "\n",
    "accuracy_scorer = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function(config, fidelity, **kwargs):\n",
    "    # Extracting support information\n",
    "    seed = kwargs[\"seed\"]\n",
    "    train_X = kwargs[\"train_X\"]\n",
    "    train_y = kwargs[\"train_y\"]\n",
    "    valid_X = kwargs[\"valid_X\"]\n",
    "    valid_y = kwargs[\"valid_y\"]\n",
    "    max_fidelity = kwargs[\"max_fidelity\"]\n",
    "    \n",
    "    if fidelity is None:\n",
    "        fidelity = max_fidelity\n",
    "    \n",
    "    start = time.time()\n",
    "    # Building model \n",
    "    model = RandomForestClassifier(\n",
    "        **config.get_dictionary(),\n",
    "        n_estimators=int(fidelity),\n",
    "        bootstrap=True,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    # Training the model on the complete training set\n",
    "    model.fit(train_X, train_y)\n",
    "    \n",
    "    # Evaluating the model on the validation set\n",
    "    valid_accuracy = accuracy_scorer(model, valid_X, valid_y)\n",
    "    cost = time.time() - start\n",
    "    \n",
    "    # Evaluating the model on the test set as additional info\n",
    "    test_accuracy = accuracy_scorer(model, test_X, test_y)\n",
    "    \n",
    "    result = {\n",
    "        \"fitness\": -valid_accuracy,  # DE/DEHB minimizes\n",
    "        \"cost\": cost,\n",
    "        \"info\": {\n",
    "            \"test_score\": test_accuracy,\n",
    "            \"fidelity\": fidelity\n",
    "        }\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all components to define the problem to be optimized. DEHB can be initialized using all these information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running DEHB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dehb import DEHB\n",
    "\n",
    "dehb = DEHB(\n",
    "    f=target_function, # Here we do not need to necessarily specify the target function, but it can still be useful to call 'run' later.\n",
    "    cs=cs, \n",
    "    dimensions=dimensions, \n",
    "    min_fidelity=min_fidelity, \n",
    "    max_fidelity=max_fidelity,\n",
    "    n_workers=1,\n",
    "    output_path=\"./temp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_function_evals = 50\n",
    "\n",
    "for _ in range(n_function_evals):\n",
    "    # Ask for the job_info, including the configuration to run and the fidelity\n",
    "    job_info = dehb.ask()\n",
    "\n",
    "    # Evaluate the configuration on the given fidelity. Here you are free to use\n",
    "    # any technique to compute the result. This job could e.g. be forwarded to\n",
    "    # a worker on your cluster (Which is not required to use Dask)\n",
    "    res = target_function(job_info[\"config\"], job_info[\"fidelity\"],\n",
    "                          # parameters as **kwargs in target_function\n",
    "                          seed=123,\n",
    "                          train_X=train_X,\n",
    "                          train_y=train_y,\n",
    "                          valid_X=valid_X,\n",
    "                          valid_y=valid_y,\n",
    "                          max_fidelity=dehb.max_fidelity)\n",
    "    \n",
    "    # When the evaluation is done, report the results back to the DEHB controller.\n",
    "    dehb.tell(job_info, res)\n",
    "\n",
    "trajectory = dehb.traj\n",
    "runtime = dehb.runtime\n",
    "history = dehb.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50 50\n",
      "\n",
      "Last evaluated configuration, \n",
      "Configuration(values={\n",
      "  'max_depth': 12,\n",
      "  'max_features': 0.6422911372304849,\n",
      "  'min_samples_leaf': 9,\n",
      "  'min_samples_split': 8,\n",
      "})got a score of -0.9722222222222222, was evaluated at a fidelity of 16.67 and took 0.015 seconds to run.\n",
      "The additional info attached: {'test_score': 1.0, 'fidelity': 16.666666666666664}\n",
      "\n",
      "Best evaluated configuration, \n",
      "Configuration(values={\n",
      "  'max_depth': 12,\n",
      "  'max_features': 0.6422911372304849,\n",
      "  'min_samples_leaf': 8,\n",
      "  'min_samples_split': 8,\n",
      "}) got an accuracy of 1.0 on the test set.\n"
     ]
    }
   ],
   "source": [
    "print(len(trajectory), len(runtime), len(history), end=\"\\n\\n\")\n",
    "\n",
    "# Last recorded function evaluation\n",
    "last_eval = history[-1]\n",
    "config, score, cost, fidelity, _info = last_eval\n",
    "\n",
    "print(\"Last evaluated configuration, \")\n",
    "print(dehb.vector_to_configspace(config), end=\"\")\n",
    "print(\"got a score of {}, was evaluated at a fidelity of {:.2f} and \"\n",
    "      \"took {:.3f} seconds to run.\".format(score, fidelity, cost))\n",
    "print(\"The additional info attached: {}\".format(_info))\n",
    "\n",
    "print()\n",
    "print(\"Best evaluated configuration, \")\n",
    "\n",
    "best_config = dehb.vector_to_configspace(dehb.inc_config)\n",
    "\n",
    "# Creating a model using the best configuration found\n",
    "model = RandomForestClassifier(\n",
    "      **best_config.get_dictionary(),\n",
    "      n_estimators=int(max_fidelity),\n",
    "      bootstrap=True,\n",
    "      random_state=seed,\n",
    ")\n",
    "# Training the model on the complete training set\n",
    "model.fit(\n",
    "      np.concatenate((train_X, valid_X)), \n",
    "      np.concatenate((train_y, valid_y))\n",
    ")\n",
    "# Evaluating the model on the held-out test set\n",
    "test_accuracy = accuracy_scorer(model, test_X, test_y)\n",
    "\n",
    "print(f\"{best_config} got an accuracy of {test_accuracy} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running DEHB for 50 function evaluations using the ask and tell interface, we can still call the `run` function in order keep optimizing without specifically using ask and tell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best evaluated configuration, \n",
      "Configuration(values={\n",
      "  'max_depth': 12,\n",
      "  'max_features': 0.5868545148270122,\n",
      "  'min_samples_leaf': 1,\n",
      "  'min_samples_split': 2,\n",
      "}) got an accuracy of 1.0 on the test set.\n"
     ]
    }
   ],
   "source": [
    "# Continuing the ask/tell run of DEHB optimization for another 10s\n",
    "trajectory, runtime, history = dehb.run(\n",
    "    total_cost=10,\n",
    "    verbose=False,\n",
    "    save_intermediate=False,\n",
    "    seed=123,\n",
    "    train_X=train_X,\n",
    "    train_y=train_y,\n",
    "    valid_X=valid_X,\n",
    "    valid_y=valid_y,\n",
    "    max_fidelity=dehb.max_fidelity\n",
    ")\n",
    "best_config = dehb.vector_to_configspace(dehb.inc_config)\n",
    "\n",
    "# Creating a model using the best configuration found\n",
    "model = RandomForestClassifier(\n",
    "    **best_config.get_dictionary(),\n",
    "    n_estimators=int(max_fidelity),\n",
    "    bootstrap=True,\n",
    "    random_state=seed,\n",
    ")\n",
    "# Training the model on the complete training set\n",
    "model.fit(\n",
    "    np.concatenate((train_X, valid_X)), \n",
    "    np.concatenate((train_y, valid_y))\n",
    ")\n",
    "# Evaluating the model on the held-out test set\n",
    "test_accuracy = accuracy_scorer(model, test_X, test_y)\n",
    "\n",
    "\n",
    "print(len(trajectory), len(runtime), len(history), end=\"\\n\\n\")\n",
    "\n",
    "# Last recorded function evaluation\n",
    "last_eval = history[-1]\n",
    "config, score, cost, fidelity, _info = last_eval\n",
    "\n",
    "print(\"Last evaluated configuration, \")\n",
    "print(dehb.vector_to_configspace(config), end=\"\")\n",
    "print(\"got a score of {}, was evaluated at a fidelity of {:.2f} and \"\n",
    "      \"took {:.3f} seconds to run.\".format(score, fidelity, cost))\n",
    "print(\"The additional info attached: {}\".format(_info))\n",
    "\n",
    "print()\n",
    "print(\"Best evaluated configuration, \")\n",
    "print(f\"{best_config} got an accuracy of {test_accuracy} on the test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
